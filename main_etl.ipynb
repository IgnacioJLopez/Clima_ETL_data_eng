{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124f73bc",
   "metadata": {},
   "source": [
    "# Trabajo Integrador - Data Engineering\n",
    "**Alumno:** Ignacio J L√≥pez\n",
    "\n",
    "## Objetivos del Proyecto\n",
    "Este proyecto implementa un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga) completo utilizando Python y Delta Lake.\n",
    "El objetivo es ingerir datos meteorol√≥gicos hist√≥ricos y predicciones, almacenarlos eficientemente y procesarlos para generar reportes anal√≠ticos.\n",
    "\n",
    "## Fuente de Datos\n",
    "Se utiliz√≥ la API p√∫blica de **Open-Meteo** debido a su robustez y disponibilidad de datos temporales granulares.\n",
    "- **Endpoint Est√°tico:** Geocoding API (Metadatos de ciudades).\n",
    "- **Endpoint Din√°mico:** Historical Weather API (Datos horarios de temperatura y precipitaci√≥n).\n",
    "\n",
    "## Estrategia de Almacenamiento\n",
    "- **Formato:** Delta Lake (por sus capacidades ACID y gesti√≥n de metadatos).\n",
    "- **Esquema:** Arquitectura Medallion simplificada (Raw -> Processed).\n",
    "- **Particionamiento:** Se particiona por fecha y ciudad en la capa Raw para optimizar la lectura incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0073bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n inicializada correctamente.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "M√≥dulo de Configuraci√≥n.\n",
    "Carga librer√≠as y variables de entorno para asegurar que las credenciales no se expongan en el c√≥digo.\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga de variables de entorno (Seguridad)\n",
    "load_dotenv()\n",
    "\n",
    "# Validaci√≥n de Configuraci√≥n\n",
    "GEO_URL = os.getenv(\"BASE_URL_GEOCODING\")\n",
    "WEATHER_URL = os.getenv(\"BASE_URL_WEATHER\")\n",
    "LAKE_PATH = \"datalake\"\n",
    "\n",
    "# Definici√≥n del alcance del proyecto\n",
    "CIUDADES_OBJETIVO = [\"Buenos Aires\", \"Ushuaia\", \"General Pico\", \"Resistencia\", \"Tilcara\"]\n",
    "\n",
    "if not GEO_URL or not WEATHER_URL:\n",
    "    raise ValueError(\"Error Cr√≠tico: Faltan variables en el archivo .env\")\n",
    "\n",
    "print(\"Configuraci√≥n inicializada correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff37ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_data_api(url, params):\n",
    "    \"\"\"\n",
    "    Gestiona la conexi√≥n con la API manejando posibles errores HTTP.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Endpoint base.\n",
    "        params (dict): Par√°metros de la consulta.\n",
    "    Returns:\n",
    "        dict: Respuesta JSON o None en caso de error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error de conexi√≥n con {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def obtener_ultima_fecha_registrada(city_id, tabla_path):\n",
    "    \"\"\"\n",
    "    Consulta el Delta Lake para determinar el punto de partida de la extracci√≥n incremental.\n",
    "    \n",
    "    Estrategia:\n",
    "    - Si la tabla no existe -> Retorna None (Indica Carga Full).\n",
    "    - Si existe -> Retorna la fecha m√°xima registrada para esa ciudad.\n",
    "    \"\"\"\n",
    "    path = f\"{LAKE_PATH}/raw/{tabla_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        dt = DeltaTable(path)\n",
    "        # Leemos solo columnas necesarias para optimizar rendimiento\n",
    "        df_existente = dt.to_pandas(columns=[\"city_id\", \"time\"])\n",
    "        df_city = df_existente[df_existente[\"city_id\"] == city_id]\n",
    "        \n",
    "        if df_city.empty:\n",
    "            return None\n",
    "        return pd.to_datetime(df_city[\"time\"]).max()\n",
    "    except Exception:\n",
    "        return None # Ante cualquier error de lectura, asumimos carga full por seguridad\n",
    "\n",
    "def extraer_metadatos_ciudades(lista_ciudades):\n",
    "    \"\"\"\n",
    "    Extracci√≥n de datos est√°ticos (Dimensiones).\n",
    "    Obtiene coordenadas y datos geogr√°ficos de las ciudades objetivo.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    print(f\"Extrayendo metadatos para {len(lista_ciudades)} ciudades...\")\n",
    "    \n",
    "    for ciudad in lista_ciudades:\n",
    "        params = {\"name\": ciudad, \"count\": 1, \"language\": \"es\", \"format\": \"json\"}\n",
    "        data = obtener_data_api(GEO_URL, params)\n",
    "        \n",
    "        if data and \"results\" in data:\n",
    "            info = data[\"results\"][0]\n",
    "            resultados.append({\n",
    "                \"city_id\": info.get(\"id\"),\n",
    "                \"name\": info.get(\"name\"),\n",
    "                \"latitude\": info.get(\"latitude\"),\n",
    "                \"longitude\": info.get(\"longitude\"),\n",
    "                \"country\": info.get(\"country\"),\n",
    "                \"population\": info.get(\"population\")\n",
    "            })\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "def extraer_datos_climaticos(df_ciudades):\n",
    "    \"\"\"\n",
    "    Extracci√≥n de datos din√°micos (Hechos) con L√≥gica Incremental.\n",
    "    \n",
    "    Decisi√≥n de Dise√±o:\n",
    "    - Se verifica la √∫ltima fecha por ciudad.\n",
    "    - Se solicita solo el delta de tiempo faltante a la API.\n",
    "    - Se guarda en modo 'append' particionado por fecha.\n",
    "    \"\"\"\n",
    "    tabla_clima = \"weather_hourly\"\n",
    "    registros_totales = 0\n",
    "    \n",
    "    print(f\"Iniciando extracci√≥n incremental de clima\")\n",
    "    \n",
    "    for _, row in df_ciudades.iterrows():\n",
    "        cid, cname = row[\"city_id\"], row[\"name\"]\n",
    "        \n",
    "        # 1. Definir ventana de tiempo (Incremental vs Full)\n",
    "        ultima_fecha = obtener_ultima_fecha_registrada(cid, tabla_clima)\n",
    "        fecha_fin = datetime.now().date()\n",
    "        \n",
    "        if ultima_fecha is None:\n",
    "            # Estrategia Full: 90 d√≠as de historia\n",
    "            fecha_inicio = fecha_fin - timedelta(days=90)\n",
    "            modo_msg = \"FULL (90 d√≠as)\"\n",
    "        else:\n",
    "            # Estrategia Incremental\n",
    "            fecha_inicio = ultima_fecha.date()\n",
    "            modo_msg = f\"INCREMENTAL (Desde {fecha_inicio})\"\n",
    "        \n",
    "        if fecha_inicio > fecha_fin:\n",
    "            continue # Datos al d√≠a\n",
    "\n",
    "        # 2. Consulta a API\n",
    "        params = {\n",
    "            \"latitude\": row[\"latitude\"],\n",
    "            \"longitude\": row[\"longitude\"],\n",
    "            \"start_date\": fecha_inicio.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": fecha_fin.strftime(\"%Y-%m-%d\"),\n",
    "            \"hourly\": \"temperature_2m,relative_humidity_2m,precipitation\",\n",
    "            \"timezone\": \"auto\"\n",
    "        }\n",
    "        \n",
    "        data = obtener_data_api(WEATHER_URL, params)\n",
    "        \n",
    "        if data and \"hourly\" in data:\n",
    "            df = pd.DataFrame(data[\"hourly\"])\n",
    "            df[\"city_id\"] = cid\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "            df[\"fecha\"] = df[\"time\"].dt.date.astype(str) # Columna para partici√≥n\n",
    "            \n",
    "            # Filtrado estricto para evitar duplicados en el borde de la fecha\n",
    "            if ultima_fecha:\n",
    "                df = df[df[\"time\"] > ultima_fecha]\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Almacenamiento Raw\n",
    "                write_deltalake(\n",
    "                    f\"{LAKE_PATH}/raw/{tabla_clima}\", \n",
    "                    df, \n",
    "                    mode=\"append\", \n",
    "                    partition_by=[\"city_id\", \"fecha\"]\n",
    "                )\n",
    "                registros_totales += len(df)\n",
    "                print(f\"   ‚úÖ {cname}: {modo_msg} -> {len(df)} registros nuevos.\")\n",
    "    \n",
    "    print(f\"Extracci√≥n finalizada. Total registros insertados: {registros_totales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5482a4",
   "metadata": {},
   "source": [
    "## Procesamiento y Transformaci√≥n de Datos\n",
    "\n",
    "En esta etapa se leen los datos crudos (\"Raw Layer\") y se aplican las siguientes transformaciones para generar valor:\n",
    "\n",
    "1.  **Limpieza:** Eliminaci√≥n de duplicados y valores nulos para asegurar la calidad del dato.\n",
    "2.  **Enriquecimiento (JOIN):** Se cruzan los datos clim√°ticos con los metadatos de las ciudades para agregar contexto geogr√°fico (Pa√≠s, Nombre).\n",
    "3.  **Ingenier√≠a de Caracter√≠sticas:** Creaci√≥n de la columna `es_alerta_clima` basada en l√≥gica condicional (Temperaturas extremas).\n",
    "4.  **Agregaci√≥n:** Se reduce la granularidad de horaria a diaria, calculando m√©tricas clave (M√≠nima, M√°xima, Promedio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a7c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo metadatos para 5 ciudades...\n",
      "Iniciando extracci√≥n incremental de clima\n",
      "   ‚úÖ Buenos Aires: FULL (90 d√≠as) -> 2184 registros nuevos.\n",
      "   ‚úÖ Ushuaia: FULL (90 d√≠as) -> 2184 registros nuevos.\n",
      "   ‚úÖ General Pico: FULL (90 d√≠as) -> 2184 registros nuevos.\n",
      "   ‚úÖ Ciudad de Resistencia: FULL (90 d√≠as) -> 2184 registros nuevos.\n",
      "   ‚úÖ Tilcara: FULL (90 d√≠as) -> 2184 registros nuevos.\n",
      "Extracci√≥n finalizada. Total registros insertados: 10920\n",
      "üè≠ Iniciando procesamiento y refinamiento...\n",
      "‚úÖ Datos procesados guardados en: datalake/processed/clima_diario\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>fecha_dia</th>\n",
       "      <th>temperature_2m_max</th>\n",
       "      <th>temperature_2m_min</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>precipitacion_total</th>\n",
       "      <th>hubo_alerta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>20.1</td>\n",
       "      <td>13.9</td>\n",
       "      <td>16.942105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>20.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>17.070833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>21.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>18.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>24.8</td>\n",
       "      <td>17.1</td>\n",
       "      <td>20.112500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>2025-10-05</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>17.658333</td>\n",
       "      <td>30.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     country          name   fecha_dia  temperature_2m_max  \\\n",
       "0  Argentina  Buenos Aires  2025-10-01                20.1   \n",
       "1  Argentina  Buenos Aires  2025-10-02                20.8   \n",
       "2  Argentina  Buenos Aires  2025-10-03                21.9   \n",
       "3  Argentina  Buenos Aires  2025-10-04                24.8   \n",
       "4  Argentina  Buenos Aires  2025-10-05                21.0   \n",
       "\n",
       "   temperature_2m_min   temp_avg  precipitacion_total  hubo_alerta  \n",
       "0                13.9  16.942105                  0.0        False  \n",
       "1                14.5  17.070833                  0.0        False  \n",
       "2                14.8  18.062500                  0.0        False  \n",
       "3                17.1  20.112500                  0.0        False  \n",
       "4                13.3  17.658333                 30.8        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def procesar_datos():\n",
    "    \"\"\"\n",
    "    Pipeline de Procesamiento.\n",
    "    Genera la capa 'Processed' con datos agregados diariamente.\n",
    "    \"\"\"\n",
    "    print(\"üè≠ Iniciando procesamiento y refinamiento...\")\n",
    "    \n",
    "    # 1. Lectura Raw\n",
    "    try:\n",
    "        df_geo = DeltaTable(f\"{LAKE_PATH}/raw/ciudades\").to_pandas()\n",
    "        df_clima = DeltaTable(f\"{LAKE_PATH}/raw/weather_hourly\").to_pandas()\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è No hay datos suficientes para procesar.\")\n",
    "        return\n",
    "\n",
    "    # 2. Limpieza\n",
    "    df_clima.drop_duplicates(subset=[\"city_id\", \"time\"], inplace=True)\n",
    "    df_clima.dropna(subset=[\"temperature_2m\"], inplace=True)\n",
    "    \n",
    "    # 3. Conversi√≥n de Tipos\n",
    "    df_clima[\"time\"] = pd.to_datetime(df_clima[\"time\"])\n",
    "    df_clima[\"fecha_dia\"] = df_clima[\"time\"].dt.date\n",
    "\n",
    "    # 4. Join (Enriquecimiento)\n",
    "    df_joined = pd.merge(\n",
    "        df_clima, \n",
    "        df_geo[[\"city_id\", \"name\", \"country\"]], \n",
    "        on=\"city_id\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # 5. L√≥gica de Negocio (Nueva Columna)\n",
    "    # Alerta si T > 30 (Calor) o T < 5 (Fr√≠o)\n",
    "    df_joined[\"es_alerta_clima\"] = np.where(\n",
    "        (df_joined[\"temperature_2m\"] > 30) | (df_joined[\"temperature_2m\"] < 5), \n",
    "        True, \n",
    "        False\n",
    "    )\n",
    "\n",
    "    # 6. Agregaciones (Resumen Diario)\n",
    "    df_resumen = df_joined.groupby([\"country\", \"name\", \"fecha_dia\"]).agg({\n",
    "        \"temperature_2m\": [\"max\", \"min\", \"mean\"],\n",
    "        \"precipitation\": \"sum\",\n",
    "        \"es_alerta_clima\": \"max\" # True si hubo alguna alerta en el d√≠a\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Aplanar columnas MultiIndex\n",
    "    df_resumen.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if col[1] else col[0] \n",
    "        for col in df_resumen.columns\n",
    "    ]\n",
    "    \n",
    "    # 7. Renombrado final\n",
    "    df_resumen.rename(columns={\n",
    "        \"temperature_2m_mean\": \"temp_avg\",\n",
    "        \"precipitation_sum\": \"precipitacion_total\",\n",
    "        \"es_alerta_clima_max\": \"hubo_alerta\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # 8. Guardado Processed (Sobreescritura para actualizar hist√≥rico)\n",
    "    ruta_proc = f\"{LAKE_PATH}/processed/clima_diario\"\n",
    "    write_deltalake(ruta_proc, df_resumen, mode=\"overwrite\", partition_by=[\"country\"])\n",
    "    \n",
    "    print(f\"‚úÖ Datos procesados guardados en: {ruta_proc}\")\n",
    "    display(df_resumen.head())\n",
    "\n",
    "# --- ORQUESTACI√ìN PRINCIPAL ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Metadatos\n",
    "    df_geo = extraer_metadatos_ciudades(CIUDADES_OBJETIVO)\n",
    "    write_deltalake(f\"{LAKE_PATH}/raw/ciudades\", df_geo, mode=\"overwrite\")\n",
    "    \n",
    "    # Paso 2: Datos Incrementales\n",
    "    extraer_datos_climaticos(df_geo)\n",
    "    \n",
    "    # Paso 3: Transformaci√≥n\n",
    "    procesar_datos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

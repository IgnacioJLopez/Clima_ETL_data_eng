{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124f73bc",
   "metadata": {},
   "source": [
    "# Trabajo Integrador - Data Engineering\n",
    "**Alumno:** Ignacio J LÃ³pez\n",
    "\n",
    "## Objetivos del Proyecto\n",
    "Este proyecto implementa un pipeline ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) completo utilizando Python y Delta Lake.\n",
    "El objetivo es ingerir datos meteorolÃ³gicos histÃ³ricos y predicciones, almacenarlos eficientemente y procesarlos para generar reportes analÃ­ticos.\n",
    "\n",
    "## Fuente de Datos\n",
    "Se utilizÃ³ la API pÃºblica de **Open-Meteo** debido a su robustez y disponibilidad de datos temporales granulares.\n",
    "- **Endpoint EstÃ¡tico:** Geocoding API (Metadatos de ciudades).\n",
    "- **Endpoint DinÃ¡mico:** Historical Weather API (Datos horarios de temperatura y precipitaciÃ³n).\n",
    "\n",
    "## Estrategia de Almacenamiento\n",
    "- **Formato:** Delta Lake (por sus capacidades ACID y gestiÃ³n de metadatos).\n",
    "- **Esquema:** Arquitectura Medallion simplificada (Raw -> Processed).\n",
    "- **Particionamiento:** Se particiona por fecha y ciudad en la capa Raw para optimizar la lectura incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0073bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MÃ³dulo de ConfiguraciÃ³n.\n",
    "Carga librerÃ­as y variables de entorno para asegurar que las credenciales no se expongan en el cÃ³digo.\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga de variables de entorno (Seguridad)\n",
    "load_dotenv()\n",
    "\n",
    "# ValidaciÃ³n de ConfiguraciÃ³n\n",
    "GEO_URL = os.getenv(\"BASE_URL_GEOCODING\")\n",
    "WEATHER_URL = os.getenv(\"BASE_URL_WEATHER\")\n",
    "LAKE_PATH = \"datalake\"\n",
    "\n",
    "# DefiniciÃ³n del alcance del proyecto\n",
    "CIUDADES_OBJETIVO = [\"Buenos Aires\", \"Ushuaia\", \"General Pico\", \"Resistencia\", \"Tilcara\"]\n",
    "\n",
    "if not GEO_URL or not WEATHER_URL:\n",
    "    raise ValueError(\"Error CrÃ­tico: Faltan variables en el archivo .env\")\n",
    "\n",
    "print(\"ConfiguraciÃ³n inicializada correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff37ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_data_api(url, params):\n",
    "    \"\"\"\n",
    "    Gestiona la conexiÃ³n con la API manejando posibles errores HTTP.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Endpoint base.\n",
    "        params (dict): ParÃ¡metros de la consulta.\n",
    "    Returns:\n",
    "        dict: Respuesta JSON o None en caso de error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error de conexiÃ³n con {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def obtener_ultima_fecha_registrada(city_id, tabla_path):\n",
    "    \"\"\"\n",
    "    Consulta el Delta Lake para determinar el punto de partida de la extracciÃ³n incremental.\n",
    "    \n",
    "    Estrategia:\n",
    "    - Si la tabla no existe -> Retorna None (Indica Carga Full).\n",
    "    - Si existe -> Retorna la fecha mÃ¡xima registrada para esa ciudad.\n",
    "    \"\"\"\n",
    "    path = f\"{LAKE_PATH}/raw/{tabla_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        dt = DeltaTable(path)\n",
    "        # Leemos solo columnas necesarias para optimizar rendimiento\n",
    "        df_existente = dt.to_pandas(columns=[\"city_id\", \"time\"])\n",
    "        df_city = df_existente[df_existente[\"city_id\"] == city_id]\n",
    "        \n",
    "        if df_city.empty:\n",
    "            return None\n",
    "        return pd.to_datetime(df_city[\"time\"]).max()\n",
    "    except Exception:\n",
    "        return None # Ante cualquier error de lectura, asumimos carga full por seguridad\n",
    "\n",
    "def extraer_metadatos_ciudades(lista_ciudades):\n",
    "    \"\"\"\n",
    "    ExtracciÃ³n de datos estÃ¡ticos (Dimensiones).\n",
    "    Obtiene coordenadas y datos geogrÃ¡ficos de las ciudades objetivo.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    print(f\"Extrayendo metadatos para {len(lista_ciudades)} ciudades...\")\n",
    "    \n",
    "    for ciudad in lista_ciudades:\n",
    "        params = {\"name\": ciudad, \"count\": 1, \"language\": \"es\", \"format\": \"json\"}\n",
    "        data = obtener_data_api(GEO_URL, params)\n",
    "        \n",
    "        if data and \"results\" in data:\n",
    "            info = data[\"results\"][0]\n",
    "            resultados.append({\n",
    "                \"city_id\": info.get(\"id\"),\n",
    "                \"name\": info.get(\"name\"),\n",
    "                \"latitude\": info.get(\"latitude\"),\n",
    "                \"longitude\": info.get(\"longitude\"),\n",
    "                \"country\": info.get(\"country\"),\n",
    "                \"population\": info.get(\"population\")\n",
    "            })\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "def extraer_datos_climaticos(df_ciudades):\n",
    "    \"\"\"\n",
    "    ExtracciÃ³n de datos dinÃ¡micos (Hechos) con LÃ³gica Incremental.\n",
    "    \n",
    "    DecisiÃ³n de DiseÃ±o:\n",
    "    - Se verifica la Ãºltima fecha por ciudad.\n",
    "    - Se solicita solo el delta de tiempo faltante a la API.\n",
    "    - Se guarda en modo 'append' particionado por fecha.\n",
    "    \"\"\"\n",
    "    tabla_clima = \"weather_hourly\"\n",
    "    registros_totales = 0\n",
    "    \n",
    "    print(f\"Iniciando extracciÃ³n incremental de clima\")\n",
    "    \n",
    "    for _, row in df_ciudades.iterrows():\n",
    "        cid, cname = row[\"city_id\"], row[\"name\"]\n",
    "        \n",
    "        # 1. Definir ventana de tiempo (Incremental vs Full)\n",
    "        ultima_fecha = obtener_ultima_fecha_registrada(cid, tabla_clima)\n",
    "        fecha_fin = datetime.now().date()\n",
    "        \n",
    "        if ultima_fecha is None:\n",
    "            # Estrategia Full: 90 dÃ­as de historia\n",
    "            fecha_inicio = fecha_fin - timedelta(days=90)\n",
    "            modo_msg = \"FULL (90 dÃ­as)\"\n",
    "        else:\n",
    "            # Estrategia Incremental\n",
    "            fecha_inicio = ultima_fecha.date()\n",
    "            modo_msg = f\"INCREMENTAL (Desde {fecha_inicio})\"\n",
    "        \n",
    "        if fecha_inicio > fecha_fin:\n",
    "            continue # Datos al dÃ­a\n",
    "\n",
    "        # 2. Consulta a API\n",
    "        params = {\n",
    "            \"latitude\": row[\"latitude\"],\n",
    "            \"longitude\": row[\"longitude\"],\n",
    "            \"start_date\": fecha_inicio.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": fecha_fin.strftime(\"%Y-%m-%d\"),\n",
    "            \"hourly\": \"temperature_2m,relative_humidity_2m,precipitation\",\n",
    "            \"timezone\": \"auto\"\n",
    "        }\n",
    "        \n",
    "        data = obtener_data_api(WEATHER_URL, params)\n",
    "        \n",
    "        if data and \"hourly\" in data:\n",
    "            df = pd.DataFrame(data[\"hourly\"])\n",
    "            df[\"city_id\"] = cid\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "            df[\"fecha\"] = df[\"time\"].dt.date.astype(str) # Columna para particiÃ³n\n",
    "            \n",
    "            # Filtrado estricto para evitar duplicados en el borde de la fecha\n",
    "            if ultima_fecha:\n",
    "                df = df[df[\"time\"] > ultima_fecha]\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Almacenamiento Raw\n",
    "                write_deltalake(\n",
    "                    f\"{LAKE_PATH}/raw/{tabla_clima}\", \n",
    "                    df, \n",
    "                    mode=\"append\", \n",
    "                    partition_by=[\"city_id\", \"fecha\"]\n",
    "                )\n",
    "                registros_totales += len(df)\n",
    "                print(f\"   âœ… {cname}: {modo_msg} -> {len(df)} registros nuevos.\")\n",
    "    \n",
    "    print(f\"ExtracciÃ³n finalizada. Total registros insertados: {registros_totales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5482a4",
   "metadata": {},
   "source": [
    "## Procesamiento y TransformaciÃ³n de Datos\n",
    "\n",
    "En esta etapa se leen los datos crudos (\"Raw Layer\") y se aplican las siguientes transformaciones para generar valor:\n",
    "\n",
    "1.  **Limpieza:** EliminaciÃ³n de duplicados y valores nulos para asegurar la calidad del dato.\n",
    "2.  **Enriquecimiento (JOIN):** Se cruzan los datos climÃ¡ticos con los metadatos de las ciudades para agregar contexto geogrÃ¡fico (PaÃ­s, Nombre).\n",
    "3.  **IngenierÃ­a de CaracterÃ­sticas:** CreaciÃ³n de la columna `es_alerta_clima` basada en lÃ³gica condicional (Temperaturas extremas).\n",
    "4.  **AgregaciÃ³n:** Se reduce la granularidad de horaria a diaria, calculando mÃ©tricas clave (MÃ­nima, MÃ¡xima, Promedio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_datos():\n",
    "    \"\"\"\n",
    "    Pipeline de Procesamiento.\n",
    "    Genera la capa 'Processed' con datos agregados diariamente.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ­ Iniciando procesamiento y refinamiento...\")\n",
    "    \n",
    "    # 1. Lectura Raw\n",
    "    try:\n",
    "        df_geo = DeltaTable(f\"{LAKE_PATH}/raw/ciudades\").to_pandas()\n",
    "        df_clima = DeltaTable(f\"{LAKE_PATH}/raw/weather_hourly\").to_pandas()\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ No hay datos suficientes para procesar.\")\n",
    "        return\n",
    "\n",
    "    # 2. Limpieza\n",
    "    df_clima.drop_duplicates(subset=[\"city_id\", \"time\"], inplace=True)\n",
    "    df_clima.dropna(subset=[\"temperature_2m\"], inplace=True)\n",
    "    \n",
    "    # 3. ConversiÃ³n de Tipos\n",
    "    df_clima[\"time\"] = pd.to_datetime(df_clima[\"time\"])\n",
    "    df_clima[\"fecha_dia\"] = df_clima[\"time\"].dt.date\n",
    "\n",
    "    # 4. Join (Enriquecimiento)\n",
    "    df_joined = pd.merge(\n",
    "        df_clima, \n",
    "        df_geo[[\"city_id\", \"name\", \"country\"]], \n",
    "        on=\"city_id\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # 5. LÃ³gica de Negocio (Nueva Columna)\n",
    "    # Alerta si T > 30 (Calor) o T < 5 (FrÃ­o)\n",
    "    df_joined[\"es_alerta_clima\"] = np.where(\n",
    "        (df_joined[\"temperature_2m\"] > 30) | (df_joined[\"temperature_2m\"] < 5), \n",
    "        True, \n",
    "        False\n",
    "    )\n",
    "\n",
    "    # 6. Agregaciones (Resumen Diario)\n",
    "    df_resumen = df_joined.groupby([\"country\", \"name\", \"fecha_dia\"]).agg({\n",
    "        \"temperature_2m\": [\"max\", \"min\", \"mean\"],\n",
    "        \"precipitation\": \"sum\",\n",
    "        \"es_alerta_clima\": \"max\" # True si hubo alguna alerta en el dÃ­a\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Aplanar columnas MultiIndex\n",
    "    df_resumen.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if col[1] else col[0] \n",
    "        for col in df_resumen.columns\n",
    "    ]\n",
    "    \n",
    "    # 7. Renombrado final\n",
    "    df_resumen.rename(columns={\n",
    "        \"temperature_2m_mean\": \"temp_avg\",\n",
    "        \"precipitation_sum\": \"precipitacion_total\",\n",
    "        \"es_alerta_clima_max\": \"hubo_alerta\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # 8. Guardado Processed (Sobreescritura para actualizar histÃ³rico)\n",
    "    ruta_proc = f\"{LAKE_PATH}/processed/clima_diario\"\n",
    "    write_deltalake(ruta_proc, df_resumen, mode=\"overwrite\", partition_by=[\"country\"])\n",
    "    \n",
    "    print(f\"âœ… Datos procesados guardados en: {ruta_proc}\")\n",
    "    display(df_resumen.head())\n",
    "\n",
    "# --- ORQUESTACIÃ“N PRINCIPAL ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Metadatos\n",
    "    df_geo = extraer_metadatos_ciudades(CIUDADES_OBJETIVO)\n",
    "    write_deltalake(f\"{LAKE_PATH}/raw/ciudades\", df_geo, mode=\"overwrite\")\n",
    "    \n",
    "    # Paso 2: Datos Incrementales\n",
    "    extraer_datos_climaticos(df_geo)\n",
    "    \n",
    "    # Paso 3: TransformaciÃ³n\n",
    "    procesar_datos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

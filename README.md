# OpenMeteo Data Pipeline & Delta Lake

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) robusto y escalable dise√±ado para la ingesta, almacenamiento y procesamiento de datos meteorol√≥gicos de alta frecuencia.

El sistema orquesta la extracci√≥n incremental de datos desde la API de Open-Meteo, gestiona un Data Lake local basado en formato **Delta Lake** y genera vistas anal√≠ticas agregadas para reportes clim√°ticos.

## üèó Arquitectura del Sistema

El flujo de datos sigue una arquitectura **Medallion (Bronze/Silver)** simplificada:

1.  **Ingesta (Extract):** Conexi√≥n a endpoints REST (Geocoding y Weather Forecast).
2.  **Raw Layer (Bronze):** Almacenamiento de datos crudos en formato Delta Lake.
    * Estrategia de **Carga Incremental** basada en control de estado ("Watermarking").
    * Particionamiento f√≠sico por `city_id` y `fecha` para optimizar lecturas.
3.  **Processing Layer (Silver/Gold):** Limpieza, enriquecimiento y agregaci√≥n.
    * Joins dimensionales (Facts + Dimensions).
    * C√°lculo de m√©tricas diarias (Min, Max, Avg).
    * Generaci√≥n de indicadores de negocio (`alertas`).

## üöÄ Stack Tecnol√≥gico

* **Lenguaje:** Python 3.10+
* **Procesamiento:** Pandas (Transformaciones en memoria).
* **Storage & ACID:** `deltalake` (Python binding para Delta Lake/Rust).
* **Ingesta:** `requests` con manejo de excepciones y retries impl√≠citos.
* **Configuraci√≥n:** `python-dotenv` para gesti√≥n segura de variables de entorno.

## ‚ú® Caracter√≠sticas Clave

### üîÑ Extracci√≥n Incremental Inteligente
A diferencia de cargas completas tradicionales, este pipeline consulta el estado actual del Data Lake antes de realizar peticiones a la API.
* **L√≥gica:** `API Request = f(Ultima_Fecha_Registrada, Fecha_Actual)`
* **Beneficio:** Minimiza el consumo de cuota de la API, reduce la latencia de red y evita la duplicidad de datos en origen.

### üõ°Ô∏è Calidad y Seguridad del Dato
* **Idempotencia:** El pipeline puede ejecutarse m√∫ltiples veces sin generar registros duplicados gracias a las restricciones de unicidad en la transformaci√≥n.
* **Aislamiento:** Las credenciales y endpoints se gestionan fuera del c√≥digo fuente mediante variables de entorno.

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone <repo-url>
    cd tp-integrador-data-eng
    ```

2.  **Instalar dependencias:**
    Se recomienda utilizar un entorno virtual.
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configuraci√≥n de Entorno:**
    El proyecto requiere un archivo `.env` en la ra√≠z con las siguientes claves:
    ```ini
    BASE_URL_GEOCODING="[https://geocoding-api.open-meteo.com/v1/search](https://geocoding-api.open-meteo.com/v1/search)"
    BASE_URL_WEATHER="[https://api.open-meteo.com/v1/forecast](https://api.open-meteo.com/v1/forecast)"
    ```

## ‚ñ∂Ô∏è Ejecuci√≥n

El pipeline est√° orquestado en un Jupyter Notebook para facilitar la visualizaci√≥n del flujo de datos paso a paso.

1.  Abrir el archivo `main_etl.ipynb`.
2.  Ejecutar las celdas secuencialmente.
3.  Verificar la creaci√≥n del directorio `./datalake` con las particiones correspondientes.

## üìä Estructura de Datos (Output)

Los datos procesados se almacenan en:
`datalake/processed/clima_diario`

Esquema resultante:
| Columna | Tipo | Descripci√≥n |
| :--- | :--- | :--- |
| `country` | string | Pa√≠s (Partition Key) |
| `name` | string | Nombre de la ciudad |
| `fecha_dia` | date | Fecha del registro |
| `temp_avg` | double | Temperatura promedio diaria |
| `precipitacion_total` | double | Acumulado de lluvias |
| `hubo_alerta` | boolean | Indicador de eventos extremos |

---
**Desarrollado por:** Ignacio J L√≥pez